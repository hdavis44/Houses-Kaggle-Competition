{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to regroup all your imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import set_config; set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÜ Le Wagon Kaggle Batch Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://wagon-public-datasets.s3.amazonaws.com/data-science-images/ML/kaggle-batch-challenge.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÜ Welcome to your first Kaggle competition!\n",
    "\n",
    "Your objective is to **submit an answer online** to the open competition - [House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)\n",
    "\n",
    "- Fortunately, you have already come across the house dataset in the bootcamp!\n",
    "- You will be semi-guided up to a **baseline model**\n",
    "- Only after will you be free to improve & refine your models\n",
    "- We will approach the problem through **pipelines** (the best practice to take!)\n",
    "\n",
    "A word on Kaggle:\n",
    "- Kaggle will rank your submission amongst all participants!\n",
    "- But don't worry, everyone is publicly removed from the leaderboard after 2 months\n",
    "- You can make up to 10 submissions per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßπ Today is the perfect day to practice keeping your long notebook **tidy** üßπ\n",
    "- \"Collapse all headings\" from the \"command palette\" (`Cmd + Shift + P`)\n",
    "- Stay idempotent (`Restart & Run All` should never crash)\n",
    "- Name and delete variables carefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Kaggle\n",
    "\n",
    "üëâ Create an account on Kaggle if you want to participate in the competition. \n",
    "\n",
    "üëâ Join the [House Prices Challenge](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data) \n",
    "\n",
    "üëâ Write down your Kaggle `username` in the [result spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) (pick the correct batch!)\n",
    "\n",
    "**Your whole class will compete as a group against the team of TAs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the challenge instructions, you should have already executed the steps to download everything you need from Kaggle into your current notebook folder:\n",
    "\n",
    "- `train.csv` is your (1460 * 81) training set containing `X` and `y`\n",
    "- `test.csv` is your (1459 * 80) testing set without the associated target `y` üòà\n",
    "- `sample_submission.csv` describing the format required to submit your answer\n",
    "- `data_description.txt` describing all columns\n",
    "\n",
    "Your goal is to predict the `y_pred` missing from your test set and submit it to discover your test_score & ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Load the training dataset in a DataFrame `data` and create your `X` and `y`. Inspect their shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
       "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
       "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
       "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
       "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
       "\n",
       "  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0   2008        WD         Normal     208500  \n",
       "1   2007        WD         Normal     181500  \n",
       "2   2008        WD         Normal     223500  \n",
       "3   2006        WD        Abnorml     140000  \n",
       "4   2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "df = pd.read_csv('data/train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Id','SalePrice'])\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 79)\n",
      "(1460,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üê£ 1. BASELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Initial feature overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "80 features is too much to deal with one-by-one for a first baseline pipeline! Let's treat them solely based on their `dtype`:\n",
    "\n",
    "‚ùì How many numerical features vs. categorical features do we have? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object     43\n",
       "int64      33\n",
       "float64     3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "X.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Store the Series `feat_categorical_nunique` containing the number of **unique values** for each categorical feature in our training set. How many unique categories are there in total ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "feat_categorical_nunique = X.select_dtypes('object').nunique()\n",
    "feat_categorical_nunique.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î If we were to `OneHotEncode` all categorical features, our feature matrix `X_preproc` would become pretty big and sparse, with almost 300 (highly correlated) features for only 1400 observations. Ideally, we should aim at feeding our model with ~50 features max (üìö Read this [rule of thumb](https://datascience.stackexchange.com/a/11480/98300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know 2 main strategies to reduce the number of categorical features post-preprocessing:\n",
    "- **[Remove](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)** features that bring too little explanation to our model. This may require statistical analysis of feature importance \n",
    "- **[Ordinally encode](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html)** (instead of one-hot-encode) categorical features into integers. However this forces a notion of \"order\" (1>2>3...) that can be detrimental if not set properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the **histogram** of number of unique value per categorical feature. Do you see some quick wins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "feat_categorical_nunique.plot(kind='hist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As a starter, what about simply **removing** all features that have **7 unique values or more**, and one-hot-encode every other? Let's keep ordinal encoding and statistical feature selection for the next iteration of our pipeline.\n",
    "\n",
    "‚ùì Store features names to OHE in a list `feat_categorical_small` below. How many features will be OHE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "feat_categorical_small = feat_categorical_nunique[feat_categorical_nunique < 7].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "len(feat_categorical_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below (and clear the cell once it passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/henry/code/hdavis44/Houses-Kaggle-Competition/tests/features_overview.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2440/1455896888.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m result = ChallengeResult('features_overview',\n\u001b[1;32m      3\u001b[0m     n=len(feat_categorical_small))\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.10.7/envs/intodata/lib/python3.10/site-packages/nbresult/__init__.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 {self.name}, one is way too big.\"\"\")\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tests\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{self.name}.pickle\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/henry/code/hdavis44/Houses-Kaggle-Competition/tests/features_overview.pickle'"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('features_overview',\n",
    "    n=len(feat_categorical_small))\n",
    "result.write(); print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Baseline pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Let's code the basic preprocessing pipeline described below. Save it under `preproc_baseline`.\n",
    "\n",
    "For categorical features\n",
    "- Simple-Impute with most frequent values\n",
    "- One-Hot-Encode features that have less than 7 unique values to start with\n",
    "- Drop all others features\n",
    "\n",
    "\n",
    "As for numerical features\n",
    "- Simple-Impute with strategy 'mean'\n",
    "- Min-Max Scale \n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>‚ÑπÔ∏è Click here for a pro tip</summary>\n",
    "\n",
    "If you are confident, you can try sklearn's shorter syntax `make_pipeline` or `make_column_transformer` instead of the longer syntax `Pipeline` or `ColumnTransformer` if you want to avoid giving names manually to every steps.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the **shape** of your preprocessed dataframe and save it to `shape_preproc_baseline` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('preproc_baseline',\n",
    "    shape=shape_preproc_baseline)\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Add estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Add a simple Decision Tree model to your `preproc_baseline` and store it to `pipe_baseline` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Cross-Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Read the Kaggle [contest evaluation rules](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview/evaluation) \n",
    "- Which performance metric do you need? Is it readily available in sklearn?\n",
    "- We will need to create our custom `sklearn.metrics.scorer` object so as to pass to any cross-validation or grid search as below\n",
    "\n",
    "\n",
    "üëâ Create a scorer called `rmsle` using [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) that can be passed as a value for the `scoring` kwarg as below:\n",
    "```python\n",
    "cross_val_score(pipe_baseline, X, y, cv=5, scoring=rmsle)\n",
    "```\n",
    "üëâ Create also the negative version `rmsle_neg` which is best when _maximized_. This will come handy later as `GridSearchCV` always tries to _maximize_ a score\n",
    "```python\n",
    "GridSearchCV(pipe_baseline, param_grid=..., cv=5, scoring=rmsle_neg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{RMSLE}(y, \\hat{y}) = \\sqrt{\\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì5-fold cross-validate your `pipe_baseline` using this metric to get a first glance at your baseline performance.    \n",
    "\n",
    "Store your mean score as `score_baseline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Predict baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Predict `y_pred_baseline` from the Kaggle `test.csv` dataset you stored in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Finally, store your CSV ready to be submitted as `submission_baseline.csv` in the `data` folder. **Carefully read** the Kaggle required format and test it below (you don't need to submit this baseline online for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "tmp = pd.read_csv(\"data/submission_baseline.csv\")\n",
    "result = ChallengeResult('submission_baseline',\n",
    "    score_baseline = score_baseline,\n",
    "    submission_shape = tmp.shape,\n",
    "    submission_columns = list(tmp.columns),\n",
    "    submission_dtypes = str(list(tmp.dtypes)),\n",
    ")\n",
    "result.write()\n",
    "print(result.check())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è 2. ITERATIONS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ üéâ Congratulation for having fully pipelined a baseline model! You will see now how much easier it is to iterate and improve performance üöÄ\n",
    "\n",
    "- Your goal is to improve your prediction and submit it online **at least 30 minutes before the Recap ‚è≥**\n",
    "- We have some suggestions for improvements below: **Pick your battles** and **incrementally** improve your pipeline as you see fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimators**\n",
    "\n",
    "- **Tree-based ensembles (must try today)**: Probably the best suited for problems with many categorical features\n",
    "- Stacking !\n",
    "- XGBoost !\n",
    "\n",
    "**Preprocessing** (once your first ensemble model works)\n",
    "\n",
    "- Ordinal Encoding of categorical features with a hidden notion of order in their values (e.g. \"bad\", \"average\", good\")\n",
    "- Statistical Feature Selection to remove useless features (avoid overfitting and reduce train time)\n",
    "- Predict `log(SalePrice)` instead?\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Iteration ‚ô≤ \n",
    "(**‚ö†Ô∏è come back here only after you have iterated on your estimators on section 2.2)**\n",
    "\n",
    "‚è© Collapse me if you don't use me!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Ordinal Encoding (1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Look at the following feature below. Couldn't it be encoded numerically in a wise manner?\n",
    "```\n",
    "ExterQual: Evaluates the quality of the material on the exterior \n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° Luckily, the `OrdinalEncoder` and its argument `categories`  allows us to do just that. Check it out below and make sure to understand how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define specific order for features\n",
    "# Note: if you change this order, it will change the output for .transform()\n",
    "feature_A_sorted_values = ['bad', 'average', 'good'] \n",
    "feature_B_sorted_values = ['dirty', 'clean', 'new']\n",
    "\n",
    "encoder = OrdinalEncoder(\n",
    "    categories=[\n",
    "        feature_A_sorted_values,\n",
    "        feature_B_sorted_values\n",
    "    ],\n",
    "    handle_unknown=\"use_encoded_value\",\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Just some random training data\n",
    "XX = [\n",
    "    ['good', 'dirty'],\n",
    "    ['bad', 'new'],\n",
    "    ['average', 'clean'],\n",
    "]\n",
    "\n",
    "encoder.fit(XX)\n",
    "\n",
    "encoder.transform([\n",
    "        ['bad', \"dirty\"],\n",
    "        [\"average\", \"clean\"],\n",
    "        ['good', 'new'],\n",
    "        ['bad', 'oooops never seen this label before']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "‚ùì **Your turn**: split your categorical preprocessor into\n",
    "\n",
    "- `preproc_ordinal` to ordinally encode **some features** of your choice\n",
    "- `preproc_nominal` to one-hot encode the other ones\n",
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- You won't be able to avoid hard-coding names and ordered values of features! Be tidy!\n",
    "- It's a good practice to sort alphabetically your features to avoid bad surprises\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Statistical Feature Selection (~30min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to remove the least interesting features, to limit overfitting and shorten training time.  \n",
    "\n",
    "üî• We will make use of sklearn's [feature selection](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) transformers directly in your pipeline!\n",
    "\n",
    "‚ùóÔ∏è We recommend you to **try only Option 1 today to start with**. Option 2 and 3 will be corrected in Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Option 1 - recommended) <font color=green>Univariate</font> feature selection based on their mutual information with target `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feel free to add a `SelectPercentile` filter at the end of your `preproc` pipeline.\n",
    "- This will filter-out features that, - taken individually - least explain your target!\n",
    "- The statistical test we recommend to pass to SelectPercentile is the `mutual_info_regression`\n",
    "\n",
    "<details>\n",
    "    <summary markdown='span'>ü§î What is mutual information? Click here!</summary>\n",
    "\n",
    "- Mutual information is a *statistical* distance between two probability distributions.\n",
    "- Correlation is a *linear* distance between two random variables.\n",
    "- Mutual information is more general and measures the reduction of uncertainty in Y after observing X.\n",
    "- On the other hand, if you already know you are working with variables that are smooth (like continuous numerical variables), sometimes correlation may tell you more about them, for instance if their relationship is monotonic.\n",
    "\n",
    "See [animation](https://twitter.com/ari_seff/status/1409296508634152964)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (option 2) <font color=green>Multivariate</font> feature selection based their combined relationship with target `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î We want to remove features that, when in combination with all the others, do not really help predict our target.\n",
    "\n",
    "1Ô∏è‚É£ To do so, remember that we can use feature [`permutation_importance`](https://scikit-learn.org/stable/modules/permutation_importance.html) metric in combination with an estimator! It trains one pipe per feature, so as to estimate which feature makes our performance score *decrease* the most when shuffling it randomly. These would be our most important features, which we don't want to remove. \n",
    "\n",
    "The best thing is, scikit-learn allows you to integrate this methodology directly into your `preproc` pipeline thanks to the [`SequentialFeatureSelector`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html) transformer: this will recursively remove least important features according to the `cross_val_score`.\n",
    "\n",
    "However, this process can take extremely long to train when you have many features.\n",
    "\n",
    "2Ô∏è‚É£ Alternatively, a faster way would be to make use of models that already output some measure of feature_importance when fitting them. For instance, Trees with gini-based `feature_importance_`, or Lasso regressions with L1 `coef_`. Again here, scikit-learn already has the [`SelectFromModel`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html) transformer to do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (option 3) <font color=green>Unsupervised</font> selection: Filter based only on the properties of `X`? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì A quick win is to remove features with the lowest variance. Think about it: a feature which only has one value is useless (and has a variance of 0).  \n",
    "- Feel free to add a [`VarianceThreshold`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html) to the end of your pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Additionally, we can check for correlation between our **numerical features** only\n",
    "\n",
    "- Use [Pearson's correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) combined with a heatmap to check visually whether some **numerical** features almost entirely correlate with others. \n",
    "- Use `VIF` from statsmodels to check for features that have the highest multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì For **ordinal features**, we can use [Spearman's rank correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient) instead to check whether some **ordinally encoded** features are almost entirely \"ordered\" similarly to others. Feel free to plot a heatmap again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now, feel free to create a \"filter\" in your pipeline that removes any feature you want beyond a given (Spearman + Pearson) correlation threshold. You'll need a custom transformer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Target engineering (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We are asked to minimize the RMS**L**E. Why don't we transform our target to directly predict its `log`?\n",
    "- Check out the histogram of the target `y`.\n",
    "- Normally distributed variables should be easier to predict with linear or parametric models. \n",
    "- Create `y_log` and your new performance metrics\n",
    "- Don't forget to take the exponent of your predictions at the end!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model Iteration ‚ôª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a ) Final version of the preproc pipeline\n",
    "‚ùì We advise you to start with a fresh definition below so you can quickly update it as need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÖFINAL SUBMISSION (submit at least 30 min before Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discover your real test score by submitting on Kaggle! \n",
    "\n",
    "üëâ Write down your test score on the [result spreadsheet here](https://docs.google.com/spreadsheets/d/1ZEBKwa_k1Ytb0WCOh-Nopq3eaezwBNu1SAqKXEXRguc/edit#gid=0) (pick the correct batch!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
